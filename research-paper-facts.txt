1.	the R is in the RSA cryptographic protocol).
2.	Deep Blue by the IBM Watson Team (Fun fact, Deep Blue beat Gary Kasparov in Chess in one of the most famous AI spectacles of the 20th century).
3.	AlphaGo by the DeepMind Team.
4.	Other paper on Game-Playing of your choosing.
2.	Write a simple one page summary of the paper covering the following:
0.	A brief summary of the paper's goals or techniques introduced (if any).
1.	A brief summary of the paper's results (if any).
Submit this as: research_review.pdf


The write up is approximately 1 page (500 words) and includes a summary of the paper (including new techniques introduced), and the key results (if any) that were achieved.
 
Research Paper Review
Paper Name: Game Tree Searching by Min / Max Approximation by Ron L. Rivest, Laboratory for Computer Science, MIT, Cambridge, MA 02139, U.S.A.; August, 1986
Summary of the paper's goals or techniques introduced.
The author of this paper introduces a technique for searching in game trees, based on the idea of approximating the min and max operators with generalized mean-value operators. 






Summary of the paper's results  

This paper introduces a new technique for searching in game trees, based on the idea of approximating the min and max operators with generalized mean-value operators.
This approximation is used to guide the selection of the next leaf node to expand, since the approximations allow one to select efficiently that leaf node upon whose value the (approximate) value at the root most highly depends.
78 R.L. RIVEST Techniques such as alpha-beta pruning and its successors [6, 2] have been essential in reducing the computational burden of exploring a game tree. Still, new techniques are needed. Nau et al. [10], after much experimentation with existing methods, assert that "A method is needed which will always expand the node that is expected to have the largest effect on the value." This paper suggests su
ch a method.
Our method, "min/max approximation," attempts to focus the computer's attention on the important lines of play. The key idea is to approximate the "min" and "max" operators with generalized mean-value operators. These are good approximations to the rain/max operators, but have continuous derivatives with respect to all arguments. This allows us to define the "expandable tip upon whose value the backed-up value at the root most heavily depends" in a

In Section 2 of this paper I present the essential results about generalized mean values that underly the new method. Then,
 in Section 3, these ideas are applied to the problem of searching game trees. 
In Section 4, I give some thoughts regarding implementation details.
 Section 5 describes our preliminary experimental results. 
Some final thoughts are presented in Section 6 .

A heuristic method is usually based on a "static evaluation function" ~ that gives an estimate b(c) of the backed-up value v(c) for a nonterminal node c. This estimate is based on "static" features of the current configuration that can be evaluated without further look-ahead (e.g. piece count and an advantage for the player to move). 

Our proposed technique requires a single static evaluator b(.). Some other methods--most notably the B* algorithm [2]--require two static evaluation functions which are upper and lower bounds on v(.). A popular approach to handling very large game trees is to select a suitable depth bound d, to estimate the values of nodes at depth d using the static evaluator, and then to compute the backed-up minimax value from the nodes at depth d using alpha-beta pruning. Given a limit on the computing time available, one can successively compute the backed-up values for depths d = 1, 2 .... , until one runs out of time, use the move determined by the last search completed. This technique is known as iterative deepening.
Similarity:
A different class of heuristics are the iterative heuristics, which "grow" the search tree one step at a time. At each step a tip node (or leaf) of the current tree is chosen, and the successors of that tip node are added to the tree. Then the values provided by the static evaluator at the new leaves are used to provide new backed-up values to the leaves' ancestors. The tree grown by an itcrative heuristic need not be of uniform depth: some branches may be searched to a much greater depth than other branches. 82 R.L. RIVEST Examples of such iterative techniques are the Berliner's B* algorithm [2], Nilsson's "arrow" method [11], Palay's probability-based method [12], and McAllester's "conspiracy number" method [7].

Details:
The general process of partially exploring a game tree by an iterative heuristic can be formalized as follows: Step 1. Initialize E to (s}, and 8E(s ) to 8(s). Step 2. While E :~ C, and while time permits, do: (a) Pick an expandable tip c of E. (b) Expand E at c. (c) Update 8E(c ) at c and the ancestors of c up to the root s, using equation (9). The major unspecified detail here is in Step 2(a)--which expandable tip c of E should we pick? The purpose of this paper is to provide a new answer to this question.
Another intriguing question is how one might gauge the accuracy of the current estimate of the value at the root, in case one wishes to use a termination condition based on accuracy instead of a termination condition based on time. We do not pursue this question is this paper.
Our idea is then to expand that tip node t which has the least penalty P(t). We add t's children to the tree, update the estimate bE(c ) for every c E A(t), and update the penalties on the edges between each c E A(t) and its children. The min/max approximation technique presented here is such a penaltybased scheme.
At each step of the iterative expansion procedure, the following steps are performed: Step 1. If b(s) = w stop---the tree has no expandable tips (i.e. E = C). Step 2. Set x to s. Step 3. While a(x)# x, set x ~--a(x). (Now x is the expandable tip node which minimizes P(x).) Step 4. Add the sucessors of x to E. Step 5. Compute bE(d ) for all successors d of x. For each expandable child d of x, initialize a(d) to d and zr(d) to 0. For each terminal child d of x, initialize a(d) to be ~o and ~-(d) to be oo. GAME TREE SEARCHING 85 Step 6. Recompute fie(x), a(x), and ~-(x) from the corresponding values at x's children. Step 7. If x = s stop, otherwise set x = f(x) and go back to Step 6.
When the algorithm terminates in the last step, then it has traced a path from the root s down to the best expandable tip x = b(s) in Es, added all the successors of x to E, and updated the rE, a, and 7r values where necessary by a traversal back up the tree from x to the root s.





Experimental Results:
5.4. Minimax search with alpha-beta pruning The implementation of minimax search with alpha-beta pruning was relatively straightforward. The usual depth-first search with a depth bound was employed. A depth bound of two ply was initially searched, and then the search was repeatedly restarted with a larger depth boundary until the time or move bound was reached (i.e. iterative deepening was used). Then the result of the last complete search was used as alpha-beta's move. (The inefficiency due to throwing away this last partial search we call "fragmentation inefficiency".) Typical search depths ran from 6 ply to more than a dozen near the end of the game. No information was carried over from a search at one depth to the search at the next. The children of a node were searched in order of their static evaluations, best-first. 5.5. Penalty-based heuristic Our implementation of the min/max heuristic worked as follows: (1) The 'reverse approximation" was used; the value computed for a node was its true backed-up rain/max value, based on the tree computed so far. (2) The penalty on an edge was computed to be 0.05 plus the absolute value of the difference between the natural logarithm of the value of the node and the natural logarithm of the value of his "best" sibling (the one with the best backed-up score, as viewed from the point of view of the person making the choice). The constant 0.05 was chosen on the basis of earlier preliminary testing. It must be admitted that the performance of the scheme was sensitive to this constant; further search is needed to make the computation of penalties more robust. 5.6. Results For each experiment, we considered 49 different starting positions. Each starting

As a summary:
First, we note that penalty-based schemes--like all iterative schemes-- requires that the tree being explored be explicitly stored. Unlike depth-first search schemes (e.g. minimax search with alpha-beta pruning), penalty-based schemes may not perform well unless they are given a large amount of memory to work with. Second, we note that the penalty-based schemes are oriented towards improving the value of the estimate ~E(s) at the root, rather than towards selecting the best move to make from the root. For example, if there is only one move to make from the root, then a penalty-based scheme may search the subtree below that move extensively, even though such exploration can't affect the decision to be made at the root. By contrast, the B* algorithm [2], another iterative search heuristic, is oriented towards making the best choice, and will not waste any time when there is only one move to be made. Third, the penalty-based schemes as presented require that a tip be expanded by generating and evaluating all of the tip's successors. Many search schemes are able to skip the evaluation of some of the successors in many cases. Fourth, we note that penalty-based schemes may appear inefficient compared to depth-first schemes, since the penalty-based schemes spend a lot of time traversing back and forth between the root and the leaves of the tree, whereas a depth-first approach will spend most of its time near the leaves. We imagine that the penalty-based schemes could be adapted to show similar efficiencies, at the cost of not always selecting the globally least-penalty tip to expand. The algorithm would be modified in Step 7 to ascend to its successor some of the time and to redescend in the tree by returning to Step 3 in the other cases. (However, if b(x) = ~o the algorithm must ascend.) The decision to redescend may be made probabilistically, perhaps as a function of the depth of 94 R.L. RIVEST x, or the change noted so far in ~-(x). For example, one might continue to redescend from the node x found in Step 3 until the number of leaves in E, exceeds the depth of x. We have not explored these alternatives. Finally, we observe that penalty-based schemes do spend some time evaluating non-optimal lines of play. However, the time spent examining such lines of play decreases as the number of non-optimal moves in the line increases, according to the weights assigned to those non-optimal moves

Advantage of this method:
We see how our "min/max approximation" heuristic will allocate resources in a sensible manner, searching shallowly in unpromising parts of the tree, and deeper in promising sections. We might also call this approach the "decreasing derivative heuristic," since the nodes are expanded in order of decreasing derivative D(s, x). It is important to note that the efficiencies exhibited by alpha-beta pruning can also appear with our scheme. Once a move has been refuted (shown to be non-optimal), its weight will increase dramatically, and further exploration down its subtree will be deferred. However, this depends on the static evaluator returning meaningful estimates. If the static evaluator were to return only constant values except at terminal positions, our scheme would perform a breadth-first search. (We observe that the scheme of McAllester [7] performs like alpha-beta search in this case.) Other penalty-based schemes are of course possible. We note two in particular: (1) If we can compute an estimate p(c,d) that the actual play would progress from configuration c to successor configuration d, given that play reaches c, then we can define the weight w(d) to be -log(p(c, d)). With this definition, the tip node to be expanded next is the tip node estimated to be most likely to be reached in play. This idea was originally proposed by Floyd (see [6]), although it does not seem to have been seriously tried. (2) If we estimate for each node ¢ the probability that c is a forced win for Max (see [9] for discussions of this idea) then we can select the tip node to expand upon which our estimate at the root depends most heavily. This can be done, in a manner similar to our min/max approximation technique, beginning with the formulas in [9]. This idea was suggested by David McAllester.
